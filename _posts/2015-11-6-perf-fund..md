---
title: Notes on Performance Funding

layout: post
---

============================
ASHE Session 11/6/2015: 

Three things that need to happen in the field:

Improved Analysis (and I don't mean more complex)

Better understanding of the micro-level processes from both a theoretical and an empirical point of view.

Incorporating more of the lessons from other fields, particularly K-12. 


Improved Analysis
-------------------

This subject: always difference-in-differences analysis for this kind of policy evaluation.

Difference in differences has kind of a checkered history in policy evaluation. It's been used in some of the most widely cited studies that have estbalished thatsmall changes in the mimnimum wage law actually increase employment. 

Diff in diff has been used in studies that show that ax gun control laws lead to more deaths, and many other solid policy evaluations. 

But it's also been used in many widely discredited studies. When using this approach, one must be cautious and think carefully about the underlying policy design.

There are two things that you MUST establish with these studies:

1. The effect cannot be attributed to anything else that happened in the state at the same time. 
2. The effect is robust to alternative defnitions of the control group.

What the best difference in differences studies do is to ruthlessly eliminate all of the possible alternative explanations until the reader is left with the conviction that it must be the policy in question that caused the changes observed. 

Here's what these studies should always do:

* Always start with the simplest difference.

* Include graphics that show the long term trends in the outcomes for both treated and control groups with implementation date noted

* Choose multiple control groups (4 or 5 possible different groups)

* Conduct falsification tests: workking with pre-treatment data, look for an effect in treated states in the years before the policy was implemented

* Working with the whole dataset, choose random implementation years and states and see how often you're picking up

* Take serial correlation really seriously. 


Better understanding of the micro-level processes from both a theoretical and an empirical point of view.
-----------------------------------------------------------------

Work in this area must be very clear about the particulars of the policy design in each state. How much money, for what outcomes, and when? In the CPPHE meeting this was discussed, but there's no such thing as "performance funding" as a policy. Instead, each state designed and implemented its own version. It's absolutely critical for studies in this area is setting a definition of implementation and using that defintion and not adoption dates as the start point. This definition itself becomes something that needs to be subject to robustness checks. 

I'm so happy that Alisa is working with Hillman and Tandberg and I'd like encourage everyone here to read her work and the work of Amanda Rutherford who has also been active in this field. There are many, many lessons to be learned from the filed of public administration with respect to accountability policies, and I recommend the work of Ken Meier among others.

One of the specific problems in this area of study is that we're still not looking at the intermediate steps that campuses would have to take in order to accomplish the goals of outcomes based funding. Dougherty and Natow among others have established the various ways in which campuses have or have not responded to the implementation of these programs. The key question is: what does the theory say about what we can expect to see changing on campuses after the implementation of the policy?  I don't think it's impossible to measure some of these intermediate steps using state-level student data systems, and this could tell us a lot about whether the theory of action embodied in this policy is actually working. 


Learning the Lessons from Other Fields
------------------------------

In K-12, they have been through this before, with a lot of very high quality work being done. In Nashville, researchers from my very own Peabody college undertook the largest and best-designed experiment in teacher performance funding ever done, with randomly assigned teachers who had the opportunity get very large (like 25-30,000 dollar) bonsues if they could improve their value added scores. After two years, the researchers found no observable difference in the value added scores between those teachers who were eligible for the bonus and those who were not. No difference, even though the treated teachers had every possible monetary incentive to improve their scores. 

This very substantial piece of evidence joins a host of evidence that was summarized by Roland Fryer like this: "The leading theory to explain our results is that students do not know the educational production function, and thus lack the know-how to transform excitement about rewards into tangible investment choices that lead to increases in achievement." 

In somewhat simpler terms, what Fryer and his colleagues have found is that if you offer to pay people for something that they already know how to do, then they are quite likely to do it. If you offer to pay people for something that they don't know how to do, It's quite unlikely that they will do it. There's not a lot of evidence that a monetary incentive (even a large one) will induce individuals to learn the skills necessary to change their performance. 

The results presented today, while they could be improved from an analytical standpoint, seem to contradict what we have been learning in K-12. This is a big surprise. 




